---
title: "p8105_hw3_hw2851"
output: github_document
---

```{r setup, include=FALSE}
library("tidyverse")
library(p8105.datasets)
data("instacart")
```

## Problem 1
```{r message=FALSE}
instacart
```

Let's first get an overview of the dataset. The instacart dataset has `r nrow(instacart)` rows. It contains `r length(instacart)` columns: `r colnames(instacart)`. Here is the first row of the dataset: `r head(instacart, n=1)`. Here is the last row of the dataset: `r tail(instacart, n=1)`. The table below shows some statistics of the key variables: the number of orders, the number of products, average number of products added to an order, the number of aisles and the number of departments.

```{r message=FALSE}
instacart %>%
  summarize(
    n_order_id = n_distinct(order_id),
    n_product_id = n_distinct(product_id),
    avg_add_to_cart_order = mean(add_to_cart_order),
    n_aisle_id = n_distinct(aisle),
    n_department = n_distinct(department_id),
  ) %>%
  knitr::kable()
```

How many aisles are there, and which aisles are the most items ordered from?

There are `r n_distinct(instacart$aisle)` aisles.  The aisles which the most orders are:
```{r message=FALSE}
instacart %>%
  count(aisle, name='n_orders') %>%
  filter(min_rank(desc(n_orders)) < 10) %>%
  arrange(desc(n_orders)) %>%
  knitr::kable()

```
 
```{r message=FALSE}
instacart_popular_aisles =
instacart %>%
  count(aisle, name='n_orders') %>%
  filter(n_orders > 10000)

ggplot(instacart_popular_aisles, aes(x = reorder(aisle, n_orders), y = n_orders)) +
  coord_flip() +
  geom_col(position = "dodge") +
  labs(
    title = "Number of orders for aisles with > 10k orders",
    x = "Aisle",
    y = "Number of orders"
  )
```
The plot above shows the number of orders for each aisle that have more than 10000 items ordered.
There are `r nrow(instacart_popular_aisles)` aisles with more than 10,000 orders. The aisles with the most numbers of orders are fresh vegetable and fresh fruits. They have more than 15k orders and excessively over other aisles.
```{r message=FALSE}
instacart %>%
  filter(aisle == c("baking ingredients", "dog food care", "packaged vegetables fruits")) %>%
  count(aisle, product_name, name="n_orders") %>%
  group_by(aisle) %>%
  filter(rank(desc(n_orders)) <= 3.0) %>%
  arrange(aisle, n_orders)
```
```{r message = FALSE}
instacart %>%
  filter(product_name %in% c("Pink Lady Apples", "Coffee Ice Cream")) %>%
  group_by(product_name, order_dow) %>%
  summarize(mean_order_hour_of_day = mean(order_hour_of_day, digit=2)) %>%
  pivot_wider(names_from=order_dow, values_from=mean_order_hour_of_day) %>%
  knitr::kable(digits = 1)
```
## Problem 2
```{r, include=FALSE}
library("tidyverse")
library(p8105.datasets)
data("brfss_smart2010")
```

```{r message=FALSE}
brfss_smart2010_clean =
brfss_smart2010 %>%
  # clean column names
  janitor::clean_names() %>%
  # only take "Overall Health" topic
  filter(topic == "Overall Health") %>%
  # make response a factor
  mutate(response = forcats::fct_relevel(response, c("Poor", "Fair", "Good", "Very good", "Excellent"))) %>%
  arrange(response)
brfss_smart2010_clean
```

```{r message=FALSE}
observed2002=
 brfss_smart2010_clean %>%
  filter(year == 2002) %>%
  group_by(locationabbr) %>%
  summarize(n_locations = n_distinct(locationdesc)) %>%
  filter(n_locations >= 7)
```
In 2002, there are 6 states that observed at 7 more locations including `r unique(observed2002$locationabbr)`
```{r message=FALSE}
observed2010=
 brfss_smart2010_clean %>%
  filter(year == 2010) %>%
  group_by(locationabbr) %>%
  summarize(n_locations = n_distinct(locationdesc)) %>%
  filter(n_locations >= 7)
```
In 2010, there are 14 states that observed at 7 more locations including `r unique(observed2010$locationabbr)`

```{r echo=FALSE} 
brfss_smart2010_clean %>%
  filter(response=="Excellent") %>%
  select(year, locationabbr, data_value) %>%
  group_by(year, locationabbr) %>%
  summarize(mean_data_value = mean(data_value)) %>%
  ggplot(aes(x = year, y = mean_data_value, color = locationabbr)) +
    geom_line()
```

```{r message=FALSE}
brfss_smart2010_clean %>%
  filter(locationabbr == "NY", year %in% c(2006, 2010)) %>%
  ggplot(aes(x = response, y = data_value)) +
  geom_violin(aes(fill = response), alpha = .5) +
  facet_grid(~year) +
  theme(legend.position = "bottom")
```

## Problem 3

```{r message=FALSE}
accel_data = read_csv('./data/accel_data.csv')%>%
  mutate(is_weekend = (day %in% c("Saturday", "Sunday"))) %>%
  pivot_longer(activity.1:activity.1440, names_to = "minute_of_day", names_prefix = "activity.", values_to = "activity_count") %>%
  mutate(minute_of_day = as.integer(minute_of_day))
```
There are 6 variables exist,including` colnames(accel_data)`, and total`r nrow(accel_data)` observations. Here is the first observation of the dataset: `r head(accel_data, n=1)`.
```{r message=FALSE}
accel_data %>%
  group_by(day_id) %>%
  summarize(total_activity = sum(activity_count)) %>%
  knitr::kable()
```

There is no any trends apparent was observed in this table. The activities looks stable from the begining to the end. There are two outliers in this dataset, they are data from day 24 and day 31. They should be excluded during analysis.

```{r message=FALSE}
accel_data %>%
  mutate(hour = ( minute_of_day - 1) %/% 60) %>%
  group_by(day_id, day, hour) %>%
  # get per hour activity
  summarize(hour_activity = sum(activity_count) ) %>%
  group_by(day, hour) %>%
  # get average per hour activity by day of week
  summarize(mean_hour_activity = mean(hour_activity) ) %>%
  ggplot(aes(x = hour, y = mean_hour_activity, color = day)) +
  geom_smooth(se = FALSE) +
  labs(
    title = "Average hourly activity count, grouped by day of the week",
    x = "Hour",
    y = "Activity count in the hour"
  )
```

This person have a lower activity at the morning and the night, in the hour 0, the mean of activities nearly at 0, as the time goes by, the mean of activities keep increasing and achieve the maximum in the afternoon and then then decreasing. There is one exception, the mean of the activity of Friday increases until hour 11 and then decreases for two hours , then keep increasing until hour 20.



